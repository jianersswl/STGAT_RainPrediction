{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5ca9ce-0d08-4a9b-afb2-082fd1e67043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianer/miniconda3/envs/PRBench/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "from data.data_split import cyclic_split\n",
    "from data.dataset import get_dataset_class,CustomTensorDataset_GBA_seq_gap, CustomTensorDataset_GBA_seq\n",
    "from data.transforms import ClassifyByThresholds\n",
    "from trainer import NIMSTrainer_Germnay_Two\n",
    "from model.swinunet_model import SwinUnet_CAM_Two\n",
    "from model.conv_lstm import ConvLSTM,ConvLSTM_Two\n",
    "from losses import *\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import sys\n",
    "import datetime\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d40860-7240-4528-be19-2669c3964b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def main(args, wandb):\n",
    "    device = set_device(args)\n",
    "    fix_seed(args.seed)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "\n",
    "    # Set experiment name and use it as process name if possible\n",
    "    experiment_name = get_experiment_name(args)\n",
    "    current_time = strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
    "    args.experiment_name = experiment_name+ \"_\" + current_time\n",
    "    experiment_name = args.experiment_name\n",
    "    \n",
    "    # print('Running Experiment'.center(30).center(80, \"=\"))\n",
    "    # print(experiment_name)\n",
    "\n",
    "    save_path = '/home/jianer/PostRainBench/3_GBA_wandb_ConvLSTM/GBA_dataset/experiment/'\n",
    "    trn_x_1 = np.load(save_path + 'X_train_period1_time_filtered.npy', mmap_mode='r')\n",
    "    trn_x_2= np.load(save_path + 'X_train_period2_time_filtered.npy', mmap_mode='r')\n",
    "    trn_y_1 = np.load(save_path + 'y_train_period1_time_filtered.npy')\n",
    "    trn_y_2 = np.load(save_path + 'y_train_period2_time_filtered.npy')\n",
    "\n",
    "    tst_x = np.load(save_path + 'X_test_period_time_filtered.npy', mmap_mode='r')\n",
    "    tst_y = np.load(save_path + 'y_test_period_time_filtered.npy')\n",
    "    vld_x = np.load(save_path + 'X_valid_period_time_filtered.npy', mmap_mode='r')\n",
    "    vld_y = np.load(save_path + 'y_valid_period_time_filtered.npy')\n",
    "\n",
    "    print('Load datasets in CPU memory successfully!')\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    train_dataset = CustomTensorDataset_GBA_seq_gap(torch.from_numpy(trn_x_1),torch.from_numpy(trn_x_2), \\\n",
    "                                                    torch.from_numpy(trn_y_1), torch.from_numpy(trn_y_2), \\\n",
    "                                                    args.rain_thresholds, sequence_length=args.seq_length, downscaling_t=4)\n",
    "    val_dataset = CustomTensorDataset_GBA_seq(torch.tensor(vld_x),torch.tensor(vld_y), args.rain_thresholds, sequence_length=args.seq_length, downscaling_t=4)\n",
    "    test_dataset = CustomTensorDataset_GBA_seq(torch.tensor(tst_x),torch.tensor(tst_y), args.rain_thresholds, sequence_length=args.seq_length, downscaling_t=4)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     for x, y, z in train_dataset:\n",
    "#         print(f'x shape: {x.shape}')\n",
    "#         print(f'y shape: {y.shape}')\n",
    "#         print(f'z shape: {z.shape}')\n",
    "#         break  # 打印一次后跳出循环\n",
    "#     for x, y, z in train_loader:\n",
    "#         print(f'x shape: {x.shape}')\n",
    "#         print(f'y shape: {y.shape}')\n",
    "#         print(f'z shape: {z.shape}')\n",
    "#         break  # 打印一次后跳出循环\n",
    "        \n",
    "    nwp_sample = torch.rand(1, 1, 84, 64, 64)\n",
    "    # set model\n",
    "    model = ConvLSTM_Two(input_data=args.input_data,\n",
    "                            window_size=args.window_size,\n",
    "                            input_dim=nwp_sample.shape[2],\n",
    "                            hidden_dim=args.hidden_dim,\n",
    "                            kernel_size=(args.kernel_size,args.kernel_size),  # hotfix: only supports single tuple of size 2\n",
    "                            num_layers=args.num_layers,\n",
    "                            num_classes=args.num_classes,\n",
    "                            batch_first=True,\n",
    "                            bias=True,\n",
    "                            return_all_layers=False)\n",
    "    \n",
    "    criterion = CrossEntropyLoss_Two(args=args,\n",
    "                                    device=device,\n",
    "                                    num_classes=args.num_classes,\n",
    "                                    experiment_name=experiment_name)\n",
    "#     if wandb.config['finetune']:\n",
    "#         checkpoint = torch.load(model_path)\n",
    "#         model.load_state_dict(checkpoint['model'], strict=True)\n",
    "        \n",
    "    dice_criterion = None\n",
    "    normalization = None\n",
    "    \n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum,\n",
    "                              weight_decay=args.wd, nesterov=args.nesterov)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    elif args.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr,\n",
    "                                  alpha=0.9, eps=1e-6)\n",
    "    elif args.optimizer == 'adadelta':\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, args.wd_ep)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=wandb.config['lr_decay_rate'], patience=10, threshold=0.0001)\n",
    "    \n",
    "    nims_trainer = NIMSTrainer_Germnay_Two(wandb, model, criterion, dice_criterion, optimizer, scheduler, device,\n",
    "                                train_loader, valid_loader, test_loader, experiment_name,\n",
    "                                args, normalization=normalization)\n",
    "    # Train model\n",
    "    nims_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1669b228-5144-4c25-bc01-8b90b19755c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "\n",
    "# 常量型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'seed': {'value': 11611801},      \n",
    "    'n_epochs': {'value': 1000},\n",
    "})\n",
    "    \n",
    "# 离散型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'train_batch_size': {\n",
    "        'values': [256]\n",
    "    },\n",
    "    'early_stop': {\n",
    "        'values': [50]\n",
    "    },\n",
    "    'tolerate_loss':{\n",
    "        'values':[1e-3]\n",
    "    },\n",
    "    'CELWeight': {\n",
    "        'values': [0,3,4]\n",
    "    },\n",
    "    'seq_length':{\n",
    "         'values': [1, 2]\n",
    "    },\n",
    "    'SFLoss': {\n",
    "        'values': [0.4, 52]\n",
    "    },\n",
    "        \n",
    "})\n",
    "\n",
    "    \n",
    "# 连续型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 1e-1\n",
    "      },\n",
    "    'alpha': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-1,\n",
    "        'max': 1e2,\n",
    "      },\n",
    "    'lr_decay_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-1,\n",
    "        'max': 8e-1,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1c23b3-7d96-4755-8d8d-df71ba6053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "     # 初始化wandb\n",
    "    ############################################################################################\n",
    "    nowtime = datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "    wandb.init(\n",
    "      project='PRBenchTest_ConvLSTM_GBA_Tansfer_2015_2022', \n",
    "      name=nowtime, \n",
    "      )\n",
    "    config = wandb.config\n",
    "    ############################################################################################\n",
    "\n",
    "    # 构建命令行参数\n",
    "    sys.argv = [\n",
    "    '--model', 'convlstm',\n",
    "    '--device', '0',\n",
    "    '--seed', str(config['seed']),\n",
    "    '--input_data', 'gdaps_kim',\n",
    "    '--num_epochs', str(config['n_epochs']),\n",
    "    '--rain_thresholds', '0.4', '52.0', '100.0',\n",
    "    '--log_dir', 'logs/logs_1106_China',\n",
    "    '--batch_size', str(config['train_batch_size']),\n",
    "    '--lr', str(config['learning_rate']),\n",
    "    '--use_two',\n",
    "    '--seq_length', str(config['seq_length']),\n",
    "    '--loss', 'ce+mse',\n",
    "    '--SFLoss', str(config['SFLoss']),\n",
    "    '--alpha', str(config['alpha']),\n",
    "    '--kernel_size', '3',\n",
    "    '--weight_version', str(config['CELWeight']),\n",
    "    '--wd_ep', '100',\n",
    "    '--custom_name', 'PRBenchTest_ConvLSTM_GBA_TSF_2015_2022'\n",
    "    ]\n",
    "    # 模型训练\n",
    "    args = parse_args(sys.argv)\n",
    "    main(args, wandb)\n",
    "    # best_model, best_loss = trainer(train_loader, valid_loader, model, wandb, device)\n",
    "    \n",
    "    # 保存模型\n",
    "    # if best_loss<0.3:\n",
    "    #     save_name = os.path.join(config['model_save_dir'], nowtime + '.ckpt')\n",
    "    #     torch.save(best_model.state_dict(), save_name)\n",
    "    #     arti_code = wandb.Artifact('ipynb', type='code')\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'SURROGATE_TRAINING_WANDB.ipynb'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMDataset.py'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMLoss.py'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMTransformer.py'))\n",
    "                                              \n",
    "    #     # arti_model = wandb.Artifact('model', type='model')\n",
    "    #     # arti_model.add_file(save_name)\n",
    "    #     wandb.log_artifact(arti_code)\n",
    "    #     wandb.log_artifact(arti_model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc5257e-d17b-4a03-bbfe-5e0c4cbd4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpanj1018\u001b[0m (\u001b[33mpanj1018-hong-kong-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f0b84b-9153-457c-abf5-1aef8e4add16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: jne96bg4\n",
      "Sweep URL: https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/sweeps/jne96bg4\n",
      "jne96bg4\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022')\n",
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99c44de-2382-4e18-90d0-741ad14b9205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p6xplvu3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tCELWeight: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tSFLoss: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 74.6301536585969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stop: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08271984100560713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_rate: 0.4592144601780157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 11611801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseq_length: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttolerate_loss: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jianer/PostRainBench/3_GBA_wandb_ConvLSTM/wandb/run-20241121_192902-p6xplvu3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/runs/p6xplvu3' target=\"_blank\">2024_11_21_192901</a></strong> to <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/sweeps/jne96bg4' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/sweeps/jne96bg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/sweeps/jne96bg4' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/sweeps/jne96bg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/runs/p6xplvu3' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_ConvLSTM_GBA_Transfer_2015_2022/runs/p6xplvu3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 11611801\n",
      "/tmp/ipykernel_1576141/1694670669.py:37: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  train_dataset = CustomTensorDataset_GBA_seq_gap(torch.from_numpy(trn_x_1),torch.from_numpy(trn_x_2), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load datasets in CPU memory successfully!\n",
      "################################################################################\n",
      "Omitting intermediate evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# wandb.agent(project='PRBenchTest_ConvLSTM_GBA_2015_2022', sweep_id='mli9ek0t', function=training, count=50)\n",
    "wandb.agent(sweep_id, training, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91808f-f386-40fb-8dd4-464e05098e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0ea34-6aa1-4492-b9fa-a8ca6174dbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0fa7b-879a-4b66-a8cb-40e0f79b041c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
