{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5ca9ce-0d08-4a9b-afb2-082fd1e67043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "from data.data_split import cyclic_split\n",
    "from data.dataset import get_dataset_class, CustomTensorDataset_GBA, CustomTensorDataset_GBA_gap\n",
    "from data.transforms import ClassifyByThresholds\n",
    "from trainer import NIMSTrainer_Germnay_Two\n",
    "from model.swinunet_model import SwinUnet_CAM_Two, SwinUnet_Two\n",
    "from model.conv_lstm import ConvLSTM,ConvLSTM_Two\n",
    "from losses import *\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import sys\n",
    "import datetime\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d40860-7240-4528-be19-2669c3964b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def main(args, wandb):\n",
    "    device = set_device(args)\n",
    "    fix_seed(args.seed)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args.seed)\n",
    "    \n",
    "\n",
    "    # Set experiment name and use it as process name if possible\n",
    "    experiment_name = get_experiment_name(args)\n",
    "    current_time = strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
    "    args.experiment_name = experiment_name+ \"_\" + current_time\n",
    "    experiment_name = args.experiment_name\n",
    "    \n",
    "    # print('Running Experiment'.center(30).center(80, \"=\"))\n",
    "    # print(experiment_name)\n",
    "\n",
    "    save_path = '/home/jianer/github_proj/PostRainBenchTest-main/3_GBA_wandb_ConvLSTM/GBA_dataset/experiment/'\n",
    "    trn_x_1 = np.load(save_path + 'X_train_period1.npy')\n",
    "    trn_x_2= np.load(save_path + 'X_train_period2.npy')\n",
    "    trn_y_1 = np.load(save_path + 'y_train_period1.npy')\n",
    "    trn_y_2 = np.load(save_path + 'y_train_period2.npy')\n",
    "\n",
    "    tst_x = np.load(save_path + 'X_test_period.npy')\n",
    "    tst_y = np.load(save_path + 'y_test_period.npy')\n",
    "    vld_x = np.load(save_path + 'X_valid_period.npy')\n",
    "    vld_y = np.load(save_path + 'y_valid_period.npy')\n",
    "\n",
    "    print('Load datasets in CPU memory successfully!')\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    train_dataset = CustomTensorDataset_GBA_gap(torch.from_numpy(trn_x_1),torch.from_numpy(trn_x_2), \\\n",
    "                                                    torch.from_numpy(trn_y_1), torch.from_numpy(trn_y_2), \\\n",
    "                                                    args.rain_thresholds, downscaling_t=4)\n",
    "    val_dataset = CustomTensorDataset_GBA(torch.from_numpy(vld_x),torch.from_numpy(vld_y), args.rain_thresholds, downscaling_t=4)\n",
    "    test_dataset = CustomTensorDataset_GBA(torch.from_numpy(tst_x),torch.from_numpy(tst_y), args.rain_thresholds, downscaling_t=4)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # for x, y, z in train_dataset:\n",
    "    #     print(f'x shape: {x.shape}')\n",
    "    #     print(f'y shape: {y.shape}')\n",
    "    #     print(f'z shape: {z.shape}')\n",
    "    #     break  # 打印一次后跳出循环\n",
    "    # for x, y, z in train_loader:\n",
    "    #     print(f'x shape: {x.shape}')\n",
    "    #     print(f'y shape: {y.shape}')\n",
    "    #     print(f'z shape: {z.shape}')\n",
    "    #     break  # 打印一次后跳出循环\n",
    "    nwp_sample = torch.rand(1, 1, 84, 64, 64)\n",
    "    # set model\n",
    "    model = SwinUnet_Two(img_size=64, in_chans=84, num_classes=len(args.rain_thresholds)+1)\n",
    "    \n",
    "    criterion = CrossEntropyLoss_Two(args=args,\n",
    "                                    device=device,\n",
    "                                    num_classes=args.num_classes,\n",
    "                                    experiment_name=experiment_name)\n",
    "    # if finetune:\n",
    "    #     checkpoint = torch.load(model_path)\n",
    "    #     model.load_state_dict(checkpoint, strict=False)\n",
    "        \n",
    "    dice_criterion = None\n",
    "    normalization = None\n",
    "    \n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum,\n",
    "                              weight_decay=args.wd, nesterov=args.nesterov)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    elif args.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr,\n",
    "                                  alpha=0.9, eps=1e-6)\n",
    "    elif args.optimizer == 'adadelta':\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, args.wd_ep)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=wandb.config['lr_decay_rate'], patience=10, threshold=0.0001)\n",
    "    \n",
    "    nims_trainer = NIMSTrainer_Germnay_Two(wandb, model, criterion, dice_criterion, optimizer, scheduler, device,\n",
    "                                train_loader, valid_loader, test_loader, experiment_name,\n",
    "                                args, normalization=normalization)\n",
    "    # Train model\n",
    "    nims_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1669b228-5144-4c25-bc01-8b90b19755c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "\n",
    "# 常量型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'seed': {'value': 11611801},      \n",
    "    'n_epochs': {'value': 1000},\n",
    "})\n",
    "    \n",
    "# 离散型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'train_batch_size': {\n",
    "        'values': [8, 16, 45, 120, 240, 384, 600]\n",
    "    },\n",
    "    'early_stop': {\n",
    "        'values': [50]\n",
    "    },\n",
    "    'tolerate_loss':{\n",
    "        'values':[1e-3]\n",
    "    },\n",
    "    'CELWeight': {\n",
    "        'values': [2,4,5,6]\n",
    "    },\n",
    "})\n",
    "\n",
    "    \n",
    "# 连续型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 1e-1\n",
    "      },\n",
    "    'alpha': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-1,\n",
    "        'max': 1e2,\n",
    "      },\n",
    "    'lr_decay_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-1,\n",
    "        'max': 8e-1,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1c23b3-7d96-4755-8d8d-df71ba6053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "     # 初始化wandb\n",
    "    ############################################################################################\n",
    "    nowtime = datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "    wandb.init(\n",
    "      project='PRBenchTest_Swin-Transformer_GBA_2015_2022', \n",
    "      name=nowtime, \n",
    "      )\n",
    "    config = wandb.config\n",
    "    ############################################################################################\n",
    "\n",
    "    # 构建命令行参数\n",
    "    sys.argv = [\n",
    "    '--model', 'SwinUnet_Two',\n",
    "    '--device', '0',\n",
    "    '--seed', str(config['seed']),\n",
    "    '--input_data', 'gdaps_kim',\n",
    "    '--num_epochs', str(config['n_epochs']),\n",
    "    '--rain_thresholds', '0.4', '52.0', '100.0',\n",
    "    '--log_dir', 'logs/logs_1111_China',\n",
    "    '--batch_size', str(config['train_batch_size']),\n",
    "    '--lr', str(config['learning_rate']),\n",
    "    '--use_two',\n",
    "    '--loss', 'ce+mse',\n",
    "    '--alpha', str(config['alpha']),\n",
    "    '--kernel_size', '3',\n",
    "    '--weight_version', str(config['CELWeight']),\n",
    "    '--wd_ep', '100',\n",
    "    '--custom_name', 'PRBenchTest_Swin-Transformer_GBA_2015_2022'\n",
    "    ]\n",
    "    # 模型训练\n",
    "    args = parse_args(sys.argv)\n",
    "    main(args, wandb)\n",
    "    # best_model, best_loss = trainer(train_loader, valid_loader, model, wandb, device)\n",
    "    \n",
    "    # 保存模型\n",
    "    # if best_loss<0.3:\n",
    "    #     save_name = os.path.join(config['model_save_dir'], nowtime + '.ckpt')\n",
    "    #     torch.save(best_model.state_dict(), save_name)\n",
    "    #     arti_code = wandb.Artifact('ipynb', type='code')\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'SURROGATE_TRAINING_WANDB.ipynb'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMDataset.py'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMLoss.py'))\n",
    "    #     arti_code.add_file(os.path.join(config['root'], 'LSMTransformer.py'))\n",
    "                                              \n",
    "    #     # arti_model = wandb.Artifact('model', type='model')\n",
    "    #     # arti_model.add_file(save_name)\n",
    "    #     wandb.log_artifact(arti_code)\n",
    "    #     wandb.log_artifact(arti_model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc5257e-d17b-4a03-bbfe-5e0c4cbd4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpanj1018\u001b[0m (\u001b[33mpanj1018-hong-kong-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f0b84b-9153-457c-abf5-1aef8e4add16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: cwebuxgw\n",
      "Sweep URL: https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/sweeps/cwebuxgw\n",
      "cwebuxgw\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='PRBenchTest_Swin-Transformer_GBA_2015_2022')\n",
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c44de-2382-4e18-90d0-741ad14b9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pg7a4zu8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tCELWeight: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 81.3473942293963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stop: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0629589129692398\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_rate: 0.4341205519442749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 11611801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttolerate_loss: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198a3df704204029948b9255b1f3072e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011116570255641516, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jianer/github_proj/PostRainBenchTest-main/3_GBA_wandb_ConvLSTM/wandb/run-20241118_155920-pg7a4zu8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/runs/pg7a4zu8' target=\"_blank\">2024_11_18_155917</a></strong> to <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/sweeps/cwebuxgw' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/sweeps/cwebuxgw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/sweeps/cwebuxgw' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/sweeps/cwebuxgw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/runs/pg7a4zu8' target=\"_blank\">https://wandb.ai/panj1018-hong-kong-university-of-science-and-technology/PRBenchTest_Swin-Transformer_GBA_2015_2022/runs/pg7a4zu8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11611801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load datasets in CPU memory successfully!\n",
      "################################################################################\n",
      "SwinTransformerSys_Two expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.0;num_classes:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianer/miniconda3/envs/PRBench/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitting intermediate evaluation on test set\n"
     ]
    }
   ],
   "source": [
    "# wandb.agent(project='PRBenchTest_Swin-Transformer_GBA_2015_2022', sweep_id='vy7tsa9d', function=training, count=50)\n",
    "wandb.agent(sweep_id, training, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91808f-f386-40fb-8dd4-464e05098e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0ea34-6aa1-4492-b9fa-a8ca6174dbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0fa7b-879a-4b66-a8cb-40e0f79b041c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
